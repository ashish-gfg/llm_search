<<<<<<< HEAD
# LLM Web Search

A Python application that searches the web for information on any topic, scrapes relevant content, and uses AI to provide accurate answers based on the gathered data.

## ğŸš€ Features

- **Web Search**: Automatically finds relevant links using Google Serper API
- **Content Scraping**: Extracts and cleans content from multiple web sources
- **AI-Powered Answers**: Uses Google Gemini AI to provide answers based on scraped content
- **Streamlit UI**: Clean, minimalistic web interface for easy interaction
- **Token Tracking**: Displays detailed token usage information for AI calls
- **Organized Logging**: Saves all scraped content in timestamped folders

## ğŸ“ Project Structure

```
llm_web_search/
â”œâ”€â”€ client.py          # Streamlit web interface
â”œâ”€â”€ main.py           # Command-line interface
â”œâ”€â”€ get_links.py      # Web search functionality
â”œâ”€â”€ scrape.py         # Content scraping and extraction
â”œâ”€â”€ cleaning.py       # Log file processing
â”œâ”€â”€ llm.py            # AI integration (Google Gemini)
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ .env             # Environment variables
â””â”€â”€ logs/            # Scraped content storage
```

## ğŸ› ï¸ Installation

### Option 1: Local Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/ashish-gfg/llm_search.git
   cd llm_search
   ```

2. **Create virtual environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   Create a `.env` file with your API keys:
   ```env
   SERPER_API_KEY=your_serper_api_key_here
   GEMINI_API_KEY=your_gemini_api_key_here
   ```

### Option 2: Docker Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/ashish-gfg/llm_search.git
   cd llm_search
   ```

2. **Build the image**
   ```bash
   docker build -t llm-web-search .
   ```

3. **Run the container**
   ```bash
   docker run -p 8501:8501 
     -e SERPER_API_KEY=your_key 
     -e GEMINI_API_KEY=your_key 
     -v $(pwd)/logs:/app/logs 
     llm-web-search
   ```

## ğŸ”‘ API Keys Required

- **Google Serper API**: Get your free API key at [serper.dev](https://serper.dev)
- **Google Gemini API**: Get your API key at [Google AI Studio](https://makersuite.google.com/app/apikey)

## ğŸ–¥ï¸ Usage

### Web Interface (Recommended)
```bash
python -m streamlit run client.py
```
Then open http://localhost:8501 in your browser.

### Command Line
Edit the `topic` variable in `main.py` and run:
```bash
python main.py
```

## ğŸ“‹ How It Works

1. **Search**: Finds relevant web links for your topic using Google Serper API
2. **Scrape**: Extracts content from multiple websites using BeautifulSoup
3. **Process**: Combines all content into a comprehensive context
4. **Answer**: Uses Google Gemini AI to provide answers based only on the scraped content
5. **Display**: Shows the AI-generated answer with token usage statistics

## ğŸ“¦ Dependencies

- `requests` - HTTP requests for web scraping
- `beautifulsoup4` - HTML parsing and content extraction
- `python-dotenv` - Environment variable management
- `google-generativeai` - Google Gemini AI integration
- `streamlit` - Web interface framework
- `numpy<2` - NumPy compatibility fix

## ğŸ¯ Example Usage

**Topic**: "latest AI developments 2024"

**Process**:
1. Searches for recent AI news and articles
2. Scrapes content from 10+ relevant websites
3. Combines all information into a comprehensive context
4. AI analyzes the content and provides a detailed summary

**Output**: A well-researched answer based on current web content, not outdated training data.

## ğŸ”§ Configuration

- **Model**: Uses `gemini-1.5-flash` by default (configurable in `llm.py`)
- **Timeout**: 10-second timeout for web requests (configurable in `scrape.py`)
- **Log Storage**: All scraped content saved in `logs/` directory with timestamps

## ğŸ“Š Token Usage

The application displays detailed token consumption:
- Input tokens (your prompt + context)
- Output tokens (AI response)
- Total tokens used
- Model information

## ğŸš¨ Important Notes

- Answers are based **only** on scraped web content
- The AI is instructed not to use its training data
- Content is saved locally for transparency and debugging
- Respects website robots.txt and implements reasonable delays

## ğŸ¤ Contributing

Feel free to submit issues, fork the repository, and create pull requests for any improvements.

## ğŸ“„ License

This project is open source and available under the MIT License.
=======
# llm_search
>>>>>>> 723965b445ba915a2b039c4d8f59d63b58f30408
